# ğŸš€ UEBlueprintBench

## ğŸ” Overview

**UEBlueprintBench** is a benchmark designed to evaluate how well various AI tools (ğŸ’¬ ChatGPT, ğŸ” Perplexity, ğŸ¤– Claude, ğŸŒŒ Gemini) assist developers in utilizing the Blueprints visual scripting language within Unreal Engine. The benchmark aims to measure accuracy, efficiency, creativity, and usability of AI-generated responses in guiding developers through Blueprint tasks.

## ğŸ¯ Goals
- âœ… Assess AI tools' effectiveness in aiding Unreal Engine developers with Blueprint visual scripting.
- ğŸ“Š Compare various AI tools based on several metrics including accuracy, efficiency, creativity, and usability.

## ğŸ“ Metrics
- ğŸ“ **Accuracy**: Correctness of AI responses in guiding users through Blueprint tasks.
- ğŸ“ **Clarity**: Quality of AI explanations, particularly in conveying complex Blueprint concepts.

## ğŸ§© Tasks
- ğŸ”¤ **Blueprint Basics**: Creating variables, functions, event handling, input/output.
- ğŸ¤– **Intermediate Systems**: AI logic, animation Blueprints, interfaces, Blueprint communication.
- ğŸŒŒ **Advanced Systems**: Procedural generation, environmental interaction, customized visual effects.
- ğŸ”§ **Troubleshooting & Debugging**: Resolving errors or inefficiencies in Blueprint scripts.
- ğŸ“ˆ **Optimization Guidance**: Suggestions for improving performance.

## ğŸ“Š Evaluation Criteria
- ğŸ“Œ **Task Success Rate**: Percentage of tasks completed successfully based on AI guidance.
- ğŸ’¬ **Comprehensibility Score**: Clarity and conciseness of the AIâ€™s response.


## ğŸ¤– Tools Benchmarked
- ğŸ’¬ **ChatGPT (OpenAI)**
- ğŸ” **Perplexity**
- ğŸ¤– **Claude (Anthropic)**
- ğŸŒŒ **Gemini (Google DeepMind)**

## ğŸ“ Output Format
- ğŸ“‚ Results are documented as markdown files within this repository.
- ğŸ“Š Comparison tables, charts, and qualitative feedback summaries are included.

## ğŸ“‚ Repository Structure
```
UEBlueprintBench/
â”œâ”€â”€ README.md                # Overview and structure of the benchmark
â”œâ”€â”€ tasks/                   # Definition of benchmark tasks (Markdown files)
â”œâ”€â”€ results/                 # AI tool performance evaluations and comparisons
â”œâ”€â”€ metrics/                 # Definitions and calculations of metrics
â””â”€â”€ feedback/                # Qualitative feedback from developers
```

## ğŸ“œ License
This project is licensed under the MIT License.

