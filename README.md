# 🚀 UEBlueprintBench

## 🔍 Overview

**UEBlueprintBench** is a benchmark designed to evaluate how well various AI tools (💬 ChatGPT, 🔎 Perplexity, 🤖 Claude, 🌌 Gemini) assist developers in utilizing the Blueprints visual scripting language within Unreal Engine. The benchmark aims to measure accuracy, efficiency, creativity, and usability of AI-generated responses in guiding developers through Blueprint tasks.

## 🎯 Goals
- ✅ Assess AI tools' effectiveness in aiding Unreal Engine developers with Blueprint visual scripting.
- 📊 Compare various AI tools based on several metrics including accuracy, efficiency, creativity, and usability.

## 📏 Metrics
- 📐 **Accuracy**: Correctness of AI responses in guiding users through Blueprint tasks.
- 📝 **Clarity**: Quality of AI explanations, particularly in conveying complex Blueprint concepts.

## 🧩 Tasks
- 🔤 **Blueprint Basics**: Creating variables, functions, event handling, input/output.
- 🤖 **Intermediate Systems**: AI logic, animation Blueprints, interfaces, Blueprint communication.
- 🌌 **Advanced Systems**: Procedural generation, environmental interaction, customized visual effects.
- 🔧 **Troubleshooting & Debugging**: Resolving errors or inefficiencies in Blueprint scripts.
- 📈 **Optimization Guidance**: Suggestions for improving performance.

## 📊 Evaluation Criteria
- 📌 **Task Success Rate**: Percentage of tasks completed successfully based on AI guidance.
- 💬 **Comprehensibility Score**: Clarity and conciseness of the AI’s response.


## 🤖 Tools Benchmarked
- 💬 **ChatGPT (OpenAI)**
- 🔎 **Perplexity**
- 🤖 **Claude (Anthropic)**
- 🌌 **Gemini (Google DeepMind)**

## 📁 Output Format
- 📂 Results are documented as markdown files within this repository.
- 📊 Comparison tables, charts, and qualitative feedback summaries are included.

## 📂 Repository Structure
```
UEBlueprintBench/
├── README.md                # Overview and structure of the benchmark
├── tasks/                   # Definition of benchmark tasks (Markdown files)
├── results/                 # AI tool performance evaluations and comparisons
├── metrics/                 # Definitions and calculations of metrics
└── feedback/                # Qualitative feedback from developers
```

## 📜 License
This project is licensed under the MIT License.

